{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "df292e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2e3199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scikit_learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scikit_learn'"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "project_data = pd.read_excel(\"(filepath)\\\\project.xlsx\")\n",
    "nlp_data_1 = pd.read_excel(\"(filepath)\\\\project_output_part_1.xlsx\")\n",
    "nlp_data_2 = pd.read_excel(\"(filepath)\\\\project_output_part_2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc2cb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have access to all our data. Next, we will join our sets together\n",
    "#First, let us sort the project dataset by the project id.\n",
    "project_data.sort_values(by = \"id\")\n",
    "#Next, we will combine our two NLP datasets and then sort it\n",
    "nlp_data_full = pd.concat((nlp_data_1, nlp_data_2), axis=0, ignore_index=True)\n",
    "nlp_data_full.sort_values(by = \"id\")\n",
    "#Checking\n",
    "for i in range(len(project_data)):\n",
    "    if (not nlp_data_full[\"id\"][i] == project_data[\"id\"][i]):\n",
    "        print(\"Error found!\" + i)\n",
    "#Drop redundant columns and then combine\n",
    "nlp_data_dropped = nlp_data_full.drop([\"id\", \"objective\"], axis=1)\n",
    "full_data = pd.concat((nlp_data_dropped, project_data), axis=1)\n",
    "full_data.to_csv(\"(filepath)\\\\project_data_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "ed4a5578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_data[\"scientific_domain\"][i] = \"no domain\"\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_data[\"problem_type\"][i] = \"no problem\"\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_data[\"expected_impact\"][i] = \"no impact\"\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_data[\"sustainability\"][i] = 0\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_data[\"fundingScheme\"][i] = \"unknown scheme\"\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n",
      "C:\\Users\\jfros\\AppData\\Local\\Temp\\ipykernel_29604\\1691130188.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"(filepath)\\\\project_data_combined.csv\")\n",
    "full_data = full_data.drop(\"Unnamed: 0\", axis = 1)\n",
    "for i in range(len(full_data)):\n",
    "    if not isinstance(full_data[\"scientific_domain\"][i], str):\n",
    "        full_data[\"scientific_domain\"][i] = \"no domain\"\n",
    "    if not isinstance(full_data[\"problem_type\"][i], str):\n",
    "        full_data[\"problem_type\"][i] = \"no problem\"\n",
    "    if not isinstance(full_data[\"expected_impact\"][i], str):\n",
    "        full_data[\"expected_impact\"][i] = \"no impact\"\n",
    "    if not isinstance(full_data[\"fundingScheme\"][i], str):\n",
    "        full_data[\"fundingScheme\"][i] = \"unknown scheme\"\n",
    "    if math.isnan(full_data[\"sustainability\"][i]):\n",
    "        full_data[\"sustainability\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "e3a036c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "#Next, we will preprocess the data to make it easier to use\n",
    "impact_categories = []\n",
    "for i in range(len(full_data)):\n",
    "    topics = full_data[\"expected_impact\"][i].split(\",\")\n",
    "    for topic in topics:\n",
    "        new_topic = topic.replace(\"[\", '')\n",
    "        new_topic = new_topic.replace(\"]\", '')\n",
    "        new_topic = new_topic.replace(\"'\", '')\n",
    "        new_topic = new_topic.replace(\" \", '')\n",
    "        new_topic = new_topic.lower()\n",
    "        if not new_topic in impact_categories:\n",
    "            impact_categories.append(new_topic)\n",
    "impact_categories.remove('sustainability')\n",
    "print(len(impact_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "80e5bd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15341\n",
      "15341\n"
     ]
    }
   ],
   "source": [
    "ohe = np.array([0] * (len(impact_categories) * len(full_data)))\n",
    "new_ohe = ohe.reshape([len(full_data), len(impact_categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "0ebc7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_data)):\n",
    "    j = 0\n",
    "    topics = full_data[\"expected_impact\"][i].split(\",\")\n",
    "    new_topics = []\n",
    "    for topic in topics:\n",
    "        new_topic = topic.replace(\"[\", '')\n",
    "        new_topic = new_topic.replace(\"]\", '')\n",
    "        new_topic = new_topic.replace(\"'\", '')\n",
    "        new_topic = new_topic.replace(\" \", '')\n",
    "        new_topic = new_topic.lower()\n",
    "        new_topics.append(new_topic)\n",
    "    new_topics\n",
    "    for j in range(len(impact_categories)):\n",
    "        if impact_categories[j] in new_topics:\n",
    "            new_ohe[i,j] = 1\n",
    "        j += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "4f707610",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [0] * len(full_data)\n",
    "for i in range(len(full_data)):\n",
    "    start_date = full_data[\"startDate\"][i]\n",
    "    startdate = start_date.split('-')\n",
    "    time1 = datetime.datetime(int(startdate[0]), int(startdate[1]), int(startdate[2]))\n",
    "    end_date = full_data[\"endDate\"][i]\n",
    "    enddate = end_date.split('-')\n",
    "    time2 = datetime.datetime(int(enddate[0]), int(enddate[1]), int(enddate[2]))\n",
    "    length = time2 - time1\n",
    "    length = str(length)[0:3]\n",
    "    length = length.replace(\" \", '')\n",
    "    lengths[i] = int(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "114c8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_data)):\n",
    "    if not full_data[\"masterCall\"][i] == full_data[\"subCall\"][i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "a3218450",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = []\n",
    "for i in range(len(full_data)):\n",
    "    if not full_data[\"fundingScheme\"][i] in schemes:\n",
    "        schemes.append(full_data[\"fundingScheme\"][i])\n",
    "ohe_schemes = np.array([0] * (len(full_data) * len(schemes)))\n",
    "ohe_schemes = ohe_schemes.reshape([len(full_data), len(schemes)])\n",
    "for i in range(len(full_data)):\n",
    "    j = 0\n",
    "    for scheme in schemes:\n",
    "        if scheme == full_data[\"fundingScheme\"][i]:\n",
    "            ohe_schemes[i, j] = 1\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "89413d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_funding = np.array([0] * len(full_data))\n",
    "for i in range(len(full_data)):\n",
    "    funding = full_data[\"ecMaxContribution\"][i]\n",
    "    funding = funding.replace(',', '.')\n",
    "    eu_funding[i] = float(funding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "09ddb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "for i in range(len(full_data)):\n",
    "    if not i in [5057, 12290, 12684]:\n",
    "        if isinstance(full_data[\"scientific_domain\"][i], str) and not full_data[\"scientific_domain\"][i].lower() in domains:\n",
    "            domains.append(full_data[\"scientific_domain\"][i].lower())\n",
    "ohe_domains = np.array([0] * (len(full_data) * len(domains)))\n",
    "ohe_domains = ohe_domains.reshape([len(full_data), len(domains)])\n",
    "for i in range(len(full_data)):\n",
    "    if not i in [5057, 12290, 12684]:\n",
    "        j = 0\n",
    "        for domain in domains:\n",
    "            if isinstance(full_data[\"scientific_domain\"][i], str) and domain == full_data[\"scientific_domain\"][i].lower():\n",
    "                ohe_domains[i, j] = 1\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "304a85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = []\n",
    "for i in range(len(full_data)):\n",
    "    if not full_data[\"problem_type\"][i].lower() in problems:\n",
    "         problems.append(full_data[\"problem_type\"][i].lower())\n",
    "ohe_problems = np.array([0] * (len(full_data) * len(problems)))\n",
    "ohe_problems = ohe_problems.reshape([len(full_data), len(problems)])\n",
    "for i in range(len(full_data)):\n",
    "    j = 0\n",
    "    for problem in problems:\n",
    "        if problem == full_data[\"problem_type\"][i].lower():\n",
    "            ohe_problems[i, j] = 1\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(problems)):\n",
    "    full_data.insert(len(full_data.columns), problems[i], ohe_problems[:,i])\n",
    "for i in range(len(impact_categories)):\n",
    "    full_data.insert(len(full_data.columns), impact_categories[i] + \"_impact\", new_ohe[:,i])\n",
    "for i in range(len(schemes)):\n",
    "    full_data.insert(len(full_data.columns), schemes[i], ohe_schemes[:,i])\n",
    "full_data.insert(27, \"Duration\", lengths)\n",
    "full_data.insert(0, \"EU_Funding\", eu_funding)\n",
    "for i in range(len(domains)):\n",
    "    full_data.insert(len(full_data.columns), domains[i] + \"_domain\", ohe_domains[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "6eff0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.to_csv(\"(filepath)\\\\project_data_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "d9490dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_columns = [\"EU_Funding\", \"sustainability\"]\n",
    "for i in range(27, len(full_data.columns)):\n",
    "    regression_columns.append(full_data.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "e8117023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EU_Funding</th>\n",
       "      <th>sustainability</th>\n",
       "      <th>fundamental research</th>\n",
       "      <th>Duration</th>\n",
       "      <th>applied research</th>\n",
       "      <th>translational research</th>\n",
       "      <th>no problem</th>\n",
       "      <th>confidential</th>\n",
       "      <th>societal_impact</th>\n",
       "      <th>economic_impact</th>\n",
       "      <th>...</th>\n",
       "      <th>musicology_domain</th>\n",
       "      <th>oceanography_domain</th>\n",
       "      <th>astronomy_domain</th>\n",
       "      <th>urban studies_domain</th>\n",
       "      <th>nuclear engineering_domain</th>\n",
       "      <th>literature_domain</th>\n",
       "      <th>material science_domain</th>\n",
       "      <th>cognitive science_domain</th>\n",
       "      <th>confidential_domain</th>\n",
       "      <th>urban planning and sustainability - interdisciplinary_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1499998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12085363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1489128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9957560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15336</th>\n",
       "      <td>75000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15337</th>\n",
       "      <td>75000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>75000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15339</th>\n",
       "      <td>75000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15340</th>\n",
       "      <td>1286125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15341 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       EU_Funding  sustainability  fundamental research  Duration  \\\n",
       "0         1499998             0.0                     1       182   \n",
       "1        12085363             0.0                     1       219   \n",
       "2         1489128             0.0                     1       182   \n",
       "3         9957560             0.0                     1       219   \n",
       "4         1500000             0.0                     1       182   \n",
       "...           ...             ...                   ...       ...   \n",
       "15336       75000             0.0                     0       214   \n",
       "15337       75000             1.0                     0       365   \n",
       "15338       75000             0.0                     0       274   \n",
       "15339       75000             0.0                     0       274   \n",
       "15340     1286125             0.0                     0       133   \n",
       "\n",
       "       applied research  translational research  no problem  confidential  \\\n",
       "0                     0                       0           0             0   \n",
       "1                     0                       0           0             0   \n",
       "2                     0                       0           0             0   \n",
       "3                     0                       0           0             0   \n",
       "4                     0                       0           0             0   \n",
       "...                 ...                     ...         ...           ...   \n",
       "15336                 1                       0           0             0   \n",
       "15337                 1                       0           0             0   \n",
       "15338                 1                       0           0             0   \n",
       "15339                 1                       0           0             0   \n",
       "15340                 1                       0           0             0   \n",
       "\n",
       "       societal_impact  economic_impact  ...  musicology_domain  \\\n",
       "0                    1                1  ...                  0   \n",
       "1                    1                0  ...                  0   \n",
       "2                    1                0  ...                  0   \n",
       "3                    1                0  ...                  0   \n",
       "4                    1                0  ...                  0   \n",
       "...                ...              ...  ...                ...   \n",
       "15336                0                1  ...                  0   \n",
       "15337                1                0  ...                  0   \n",
       "15338                1                0  ...                  0   \n",
       "15339                1                1  ...                  0   \n",
       "15340                1                0  ...                  0   \n",
       "\n",
       "       oceanography_domain  astronomy_domain  urban studies_domain  \\\n",
       "0                        0                 0                     0   \n",
       "1                        0                 0                     0   \n",
       "2                        0                 0                     0   \n",
       "3                        0                 0                     0   \n",
       "4                        0                 0                     0   \n",
       "...                    ...               ...                   ...   \n",
       "15336                    0                 0                     0   \n",
       "15337                    0                 0                     0   \n",
       "15338                    0                 0                     0   \n",
       "15339                    0                 0                     0   \n",
       "15340                    0                 0                     0   \n",
       "\n",
       "       nuclear engineering_domain  literature_domain  material science_domain  \\\n",
       "0                               0                  0                        0   \n",
       "1                               0                  0                        0   \n",
       "2                               0                  0                        0   \n",
       "3                               0                  0                        0   \n",
       "4                               0                  0                        0   \n",
       "...                           ...                ...                      ...   \n",
       "15336                           0                  0                        0   \n",
       "15337                           0                  0                        0   \n",
       "15338                           0                  0                        0   \n",
       "15339                           0                  0                        0   \n",
       "15340                           0                  0                        0   \n",
       "\n",
       "       cognitive science_domain  confidential_domain  \\\n",
       "0                             0                    0   \n",
       "1                             0                    0   \n",
       "2                             0                    0   \n",
       "3                             0                    0   \n",
       "4                             0                    0   \n",
       "...                         ...                  ...   \n",
       "15336                         0                    0   \n",
       "15337                         0                    0   \n",
       "15338                         0                    0   \n",
       "15339                         0                    0   \n",
       "15340                         0                    0   \n",
       "\n",
       "       urban planning and sustainability - interdisciplinary_domain  \n",
       "0                                                      0             \n",
       "1                                                      0             \n",
       "2                                                      0             \n",
       "3                                                      0             \n",
       "4                                                      0             \n",
       "...                                                  ...             \n",
       "15336                                                  0             \n",
       "15337                                                  0             \n",
       "15338                                                  0             \n",
       "15339                                                  0             \n",
       "15340                                                  0             \n",
       "\n",
       "[15341 rows x 184 columns]"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_data = full_data[regression_columns]\n",
    "regression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "fb8b44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "shuffled_data = regression_data.sample(frac=1).reset_index(drop=True)\n",
    "training_set = shuffled_data[0:12272]\n",
    "test_set = shuffled_data[12273:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "28ec706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictors = training_set[training_set.columns[1:]]\n",
    "training_response = training_set[\"EU_Funding\"]\n",
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(training_predictors, training_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "fd93d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5328452724590515e+21\n"
     ]
    }
   ],
   "source": [
    "test_predictors = test_set[test_set.columns[1:]]\n",
    "predictions = regression.predict(test_predictors)\n",
    "mse = 0\n",
    "for i in range(len(test_set)):\n",
    "    mse = mse + (test_set[\"EU_Funding\"][i + 12273] - predictions[i])**2\n",
    "mse = mse / len(test_set)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "e013fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_funding = np.array([0.0] * len(regression_data))\n",
    "for i in range(len(regression_data)):\n",
    "    log_funding[i] = math.log(regression_data[\"EU_Funding\"][i])\n",
    "regression_data.insert(1, \"log_funding\", log_funding)\n",
    "random.seed(0)\n",
    "shuffled_data = regression_data.sample(frac=1).reset_index(drop=True)\n",
    "training_set = shuffled_data[0:12272]\n",
    "test_set = shuffled_data[12273:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "f08bf1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictors = training_set[training_set.columns[2:]]\n",
    "training_response = training_set[\"log_funding\"]\n",
    "regression = linear_model.LinearRegression()\n",
    "regression.fit(training_predictors, training_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "9f3c5dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- fund_cats\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[776], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_predictors \u001b[38;5;241m=\u001b[39m test_set[test_set\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m2\u001b[39m:]]\n\u001b[1;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mregression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_predictors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    516\u001b[0m ):\n\u001b[0;32m    517\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:506\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    502\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m     )\n\u001b[1;32m--> 506\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- fund_cats\n"
     ]
    }
   ],
   "source": [
    "test_predictors = test_set[test_set.columns[2:]]\n",
    "predictions = regression.predict(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "f73f6fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High Cost' 'High Cost' 'High Cost' ... 'High Cost' 'High Cost'\n",
      " 'High Cost']\n",
      "12273    15.604960\n",
      "12274    12.349194\n",
      "12275    11.967955\n",
      "12276    14.730229\n",
      "12277    12.107092\n",
      "           ...    \n",
      "15336    13.152310\n",
      "15337    14.732364\n",
      "15338    14.692015\n",
      "15339    16.223855\n",
      "15340    12.015590\n",
      "Name: log_funding, Length: 3068, dtype: float64\n",
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[733], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m36\u001b[39m:\n\u001b[1;32m----> 9\u001b[0m         mse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m test_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEU_Funding\u001b[39m\u001b[38;5;124m\"\u001b[39m][i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m12273\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mabs\u001b[39m(math\u001b[38;5;241m.\u001b[39mexp(predictions[i]) \u001b[38;5;241m-\u001b[39m test_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEU_Funding\u001b[39m\u001b[38;5;124m\"\u001b[39m][i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m12273\u001b[39m])\n\u001b[0;32m     10\u001b[0m mse \u001b[38;5;241m=\u001b[39m mse \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(test_set) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(mse)\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not str"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(test_set[\"log_funding\"])\n",
    "np.delete(predictions, 36)\n",
    "test_set.drop([36 + 12273])\n",
    "mse = 0\n",
    "for i in range(len(test_set)):\n",
    "    print(i)\n",
    "    if not i == 36: #Value causes overflow in the following line, we chose to remove it\n",
    "        mse += abs(math.exp(predictions[i]) - test_set[\"EU_Funding\"][i + 12273]) * abs(math.exp(predictions[i]) - test_set[\"EU_Funding\"][i + 12273])\n",
    "mse = mse / (len(test_set) - 1)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "afdd70c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "funding_categories = []\n",
    "for i in range(len(regression_data)):\n",
    "    if regression_data[\"EU_Funding\"][i] > 20000000:\n",
    "        funding_categories.append(\"Massive Cost\")\n",
    "    elif regression_data[\"EU_Funding\"][i] > 200000:\n",
    "        funding_categories.append(\"High Cost\")\n",
    "    else:\n",
    "        funding_categories.append(\"Low Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "48577f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_data.insert(2, \"fund_cats\", funding_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "0484da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "shuffled_data = regression_data.sample(frac=1).reset_index(drop=True)\n",
    "training_set = shuffled_data[0:12272]\n",
    "test_set = shuffled_data[12273:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "c372c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pred = training_set[regression_columns[3:]]\n",
    "training_resp = training_set[\"fund_cats\"]\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier = classifier.fit(training_pred, training_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "7f3b2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = test_set[regression_columns[3:]]\n",
    "training_resp = training_set[\"fund_cats\"]\n",
    "class_predictions = classifier.predict(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "61342110",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_preds = training_set[training_set[\"fund_cats\"].isin([\"Low Cost\"])]\n",
    "lc_preds = lc_preds[lc_preds.columns[3:]]\n",
    "hc_preds = training_set[training_set[\"fund_cats\"].isin([\"High Cost\"])]\n",
    "hc_preds = hc_preds[hc_preds.columns[3:]]\n",
    "mc_preds = training_set[training_set[\"fund_cats\"].isin([\"Massive Cost\"])]\n",
    "mc_preds = mc_preds[mc_preds.columns[3:]]\n",
    "lc_resps = training_set[training_set[\"fund_cats\"].isin([\"Low Cost\"])]\n",
    "lc_resps = lc_resps[lc_resps.columns[1]]\n",
    "hc_resps = training_set[training_set[\"fund_cats\"].isin([\"High Cost\"])]\n",
    "hc_resps = hc_resps[hc_resps.columns[1]]\n",
    "mc_resps = training_set[training_set[\"fund_cats\"].isin([\"Massive Cost\"])]\n",
    "mc_resps = mc_resps[mc_resps.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "d453df45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_regression = linear_model.LinearRegression()\n",
    "lc_regression.fit(lc_preds, lc_resps)\n",
    "hc_regression = linear_model.LinearRegression()\n",
    "hc_regression.fit(hc_preds, hc_resps)\n",
    "mc_regression = linear_model.LinearRegression()\n",
    "mc_regression.fit(mc_preds, mc_resps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "8b89223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_test = test_set[class_predictions == \"Low Cost\"]\n",
    "lc_test = lc_test[lc_test.columns[3:]]\n",
    "hc_test = test_set[class_predictions == \"High Cost\"]\n",
    "hc_test = hc_test[hc_test.columns[3:]]\n",
    "mc_test = test_set[class_predictions == \"Massive Cost\"]\n",
    "mc_test = mc_test[mc_test.columns[3:]]\n",
    "lc_true = test_set[class_predictions == \"Low Cost\"]\n",
    "lc_true = lc_true[lc_true.columns[1]]\n",
    "hc_true = test_set[class_predictions == \"High Cost\"]\n",
    "hc_true = hc_true[hc_true.columns[1]]\n",
    "mc_true = test_set[class_predictions == \"Massive Cost\"]\n",
    "mc_true = mc_true[mc_true.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "a6539185",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_predictions = lc_regression.predict(lc_test)\n",
    "hc_predictions = hc_regression.predict(hc_test)\n",
    "mc_predictions = mc_regression.predict(mc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "0d059808",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_test = lc_test.reset_index()\n",
    "hc_test = hc_test.reset_index()\n",
    "mc_test = mc_test.reset_index()\n",
    "lc_true = lc_true.reset_index()\n",
    "hc_true = hc_true.reset_index()\n",
    "mc_true = mc_true.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "c091b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 0\n",
    "lc_true[\"log_funding\"][i]\n",
    "skipped = 0\n",
    "for i in range(len(lc_predictions)):\n",
    "    if lc_predictions[i] < 100:\n",
    "        mse += abs(math.exp(lc_predictions[i]) - math.exp(lc_true[\"log_funding\"][i])) * abs(math.exp(lc_predictions[i]) - math.exp(lc_true[\"log_funding\"][i]))\n",
    "    else:\n",
    "        skipped += 1\n",
    "for i in range(len(hc_predictions)):\n",
    "    if hc_predictions[i] < 100:\n",
    "        mse += abs(math.exp(hc_predictions[i]) - math.exp(hc_true[\"log_funding\"][i])) * abs(math.exp(hc_predictions[i]) - math.exp(hc_true[\"log_funding\"][i]))\n",
    "    else:\n",
    "        skipped += 1\n",
    "for i in range(len(mc_predictions)):\n",
    "    if mc_predictions[i] < 100:\n",
    "        mse += abs(math.exp(mc_predictions[i]) - math.exp(mc_true[\"log_funding\"][i])) * abs(math.exp(mc_predictions[i]) - math.exp(mc_true[\"log_funding\"][i]))\n",
    "    else:\n",
    "        skipped += 1\n",
    "mse = mse / (len(test_set) - skipped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
