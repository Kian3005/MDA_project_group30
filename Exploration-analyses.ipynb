{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfp = pd.read_excel(r\"C:\\Users\\lieve\\Documents\\School\\2024 - 2025\\Semester 2\\Modern Data Analytics [G0Z39a]\\Project\\cordis-HORIZONprojects-xlsx\\project.xlsx\")\n",
    "dfo = pd.read_excel(r\"C:\\Users\\lieve\\Documents\\School\\2024 - 2025\\Semester 2\\Modern Data Analytics [G0Z39a]\\Project\\cordis-HORIZONprojects-xlsx\\organization.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfoutput1 = pd.read_excel(r\"C:\\Users\\lieve\\Documents\\School\\2024 - 2025\\Semester 2\\Modern Data Analytics [G0Z39a]\\Project\\project_output_part_1.xlsx\")\n",
    "dfoutput2 = pd.read_excel(r\"C:\\Users\\lieve\\Documents\\School\\2024 - 2025\\Semester 2\\Modern Data Analytics [G0Z39a]\\Project\\project_output_part_2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e437f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_output= pd.concat([dfoutput1, dfoutput2], axis=0, ignore_index=True, verify_integrity=True)\n",
    "print(df_full_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfp.shape)\n",
    "print(dfo.shape)\n",
    "# One to one merge dfp and df_full_output\n",
    "newdf = dfp.merge(df_full_output, on=\"id\")\n",
    "### One to many merge newdf and dfo, except 100249 rows in fulldf\n",
    "fulldf = newdf.merge(dfo, left_on=\"id\", right_on=\"projectID\", validate=\"one_to_many\")\n",
    "print(fulldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf.head())\n",
    "print(fulldf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete active and nature (empty)\n",
    "fulldf = fulldf.drop(columns=[\"active\", \"nature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf.head())\n",
    "print(fulldf.info())\n",
    "print(fulldf.shape)\n",
    "print(fulldf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35469ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save memory and speed ML training save categorical variables as category datatype\n",
    "print(fulldf[\"status\"].dtype)\n",
    "print(fulldf[\"status\"].nbytes)\n",
    "fulldf[\"status\"] = fulldf[\"status\"].astype(\"category\")\n",
    "print(fulldf[\"status\"].nbytes)\n",
    "fulldf[\"status\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a926df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used to include features about the amount of coordinators, participants, ... of a project\n",
    "print(fulldf[\"role\"].dtype)\n",
    "print(fulldf[\"role\"].nbytes)\n",
    "fulldf[\"role\"] = fulldf[\"role\"].astype(\"category\")\n",
    "print(fulldf[\"role\"].nbytes)\n",
    "fulldf[\"role\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 different levels, maybe add as a feature?\n",
    "print(fulldf[\"legalBasis\"].dtype)\n",
    "print(fulldf[\"legalBasis\"].nbytes)\n",
    "fulldf[\"legalBasis\"] = fulldf[\"legalBasis\"].astype(\"category\")\n",
    "print(fulldf[\"legalBasis\"].nbytes)\n",
    "fulldf[\"legalBasis\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many, only use our topic feature\n",
    "print(fulldf[\"topics\"].dtype)\n",
    "print(fulldf[\"topics\"].nbytes)\n",
    "fulldf[\"topics\"] = fulldf[\"topics\"].astype(\"category\")\n",
    "print(fulldf[\"topics\"].nbytes)\n",
    "fulldf[\"topics\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one category, will be dropped as it is not informative\n",
    "print(fulldf[\"frameworkProgramme\"].dtype)\n",
    "print(fulldf[\"frameworkProgramme\"].nbytes)\n",
    "fulldf[\"frameworkProgramme\"] = fulldf[\"frameworkProgramme\"].astype(\"category\")\n",
    "print(fulldf[\"frameworkProgramme\"].nbytes)\n",
    "fulldf[\"frameworkProgramme\"].dtype\n",
    "fulldf = fulldf.drop(columns=[\"frameworkProgramme\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also too many levels, looks like topics (test dubble columns later)\n",
    "print(fulldf[\"masterCall\"].dtype)\n",
    "print(fulldf[\"masterCall\"].nbytes)\n",
    "fulldf[\"masterCall\"] = fulldf[\"masterCall\"].astype(\"category\")\n",
    "print(fulldf[\"masterCall\"].nbytes)\n",
    "fulldf[\"masterCall\"].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1880481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also too many levels, looks like topics and mastercall (test dubble columns later)\n",
    "print(fulldf[\"subCall\"].dtype)\n",
    "print(fulldf[\"subCall\"].nbytes)\n",
    "fulldf[\"subCall\"] = fulldf[\"subCall\"].astype(\"category\")\n",
    "print(fulldf[\"subCall\"].nbytes)\n",
    "fulldf[\"subCall\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de652b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +30 cat's but looks like there is a lot of subschemes (ERC, EURATOM, HORIZON, EI...)\n",
    "print(fulldf[\"fundingScheme\"].dtype)\n",
    "print(fulldf[\"fundingScheme\"].nbytes)\n",
    "fulldf[\"fundingScheme\"] = fulldf[\"fundingScheme\"].astype(\"category\")\n",
    "print(fulldf[\"fundingScheme\"].nbytes)\n",
    "fulldf[\"fundingScheme\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible feature if we want to do something with the originizations activity types\n",
    "print(fulldf[\"activityType\"].dtype)\n",
    "print(fulldf[\"activityType\"].nbytes)\n",
    "fulldf[\"activityType\"] = fulldf[\"activityType\"].astype(\"category\")\n",
    "print(fulldf[\"activityType\"].nbytes)\n",
    "fulldf[\"activityType\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96995fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe if we want to group variables in region and then check influence on funding? Or put on a worldmap?\n",
    "# Can also use the column nutscode, EU's nomenclature for deviding in statistical regions, see next cell\n",
    "print(fulldf[\"country\"].dtype)\n",
    "print(fulldf[\"country\"].nbytes)\n",
    "fulldf[\"country\"] = fulldf[\"country\"].astype(\"category\")\n",
    "print(fulldf[\"country\"].nbytes)\n",
    "fulldf[\"country\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot, better to group on country level maybe?\n",
    "print(fulldf[\"nutsCode\"].dtype)\n",
    "print(fulldf[\"nutsCode\"].nbytes)\n",
    "fulldf[\"nutsCode\"] = fulldf[\"nutsCode\"].astype(\"category\")\n",
    "print(fulldf[\"nutsCode\"].nbytes)\n",
    "fulldf[\"nutsCode\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36433a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf[\"order\"].dtype)\n",
    "print(fulldf[\"order\"].nbytes)\n",
    "fulldf[\"order\"] = fulldf[\"order\"].astype(\"category\")\n",
    "print(fulldf[\"order\"].nbytes)\n",
    "fulldf[\"order\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50259427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf[\"main_topics\"].dtype)\n",
    "print(fulldf[\"main_topics\"].nbytes)\n",
    "fulldf[\"main_topics\"] = fulldf[\"main_topics\"].astype(\"category\")\n",
    "print(fulldf[\"main_topics\"].nbytes)\n",
    "fulldf[\"main_topics\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still a lot, maybe group further like 'urban planning', 'urban studies' as urban\n",
    "print(fulldf[\"scientific_domain\"].dtype)\n",
    "print(fulldf[\"scientific_domain\"].nbytes)\n",
    "fulldf[\" scientific_domain\"] = fulldf[\"scientific_domain\"].astype(\"category\")\n",
    "print(fulldf[\"scientific_domain\"].nbytes)\n",
    "fulldf[\" scientific_domain\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba165709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some with upper case some without, convert this to capitalized categories\n",
    "print(fulldf[\"problem_type\"].dtype)\n",
    "print(fulldf[\"problem_type\"].nbytes)\n",
    "fulldf[\"problem_type\"] = fulldf[\"problem_type\"].astype(\"category\")\n",
    "print(fulldf[\"problem_type\"].nbytes)\n",
    "fulldf[\"problem_type\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6baadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some with 1, some 2 ... maybe make 1 or try to have all 2 or 3 for example? (the more the more categories)\n",
    "print(fulldf[\"expected_impact\"].dtype)\n",
    "print(fulldf[\"expected_impact\"].nbytes)\n",
    "fulldf[\"expected_impact\"] = fulldf[\"expected_impact\"].astype(\"category\")\n",
    "print(fulldf[\"expected_impact\"].nbytes)\n",
    "fulldf[\"expected_impact\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145cfabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns we will probably not use for prediction\n",
    "# Geographical columns can be used later if we want to do geographical clustering for example\n",
    "fulldf = fulldf.drop(columns=[\"acronym\", \"title\", \"totalCost_x\", \"topics\", \"objective_x\", \"rcn_x\", \"grantDoi\", \"contentUpdateDate_x\", \\\n",
    "                              \"projectID\", \"projectAcronym\", \"organisationID\", \"vatNumber\", \"name\", \"shortName\", \"street\", \"postCode\", \\\n",
    "                              \"city\", \"geolocation\", \"organizationURL\", \"contactForm\",\"contentUpdateDate_y\", \"rcn_y\", \"totalCost_y\", \\\n",
    "                              \"ecContribution\", \"netEcContribution\", \"ecSignatureDate\", \"masterCall\", \"subCall\", \" scientific_domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad503dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf.head())\n",
    "print(fulldf.info())\n",
    "print(fulldf.shape)\n",
    "print(fulldf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf[\"ecMaxContribution\"] = fulldf[\"ecMaxContribution\"].str.replace(',', '.', regex=False).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc23ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf.head())\n",
    "print(fulldf.info())\n",
    "print(fulldf.shape)\n",
    "print(fulldf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b443dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf[\"SME\"] = fulldf[\"SME\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ef7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf[\"SME\"] = fulldf[\"SME\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf[\"startDate\"] = pd.to_datetime(fulldf[\"startDate\"])\n",
    "fulldf[\"endDate\"] = pd.to_datetime(fulldf[\"endDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fulldf.head())\n",
    "print(fulldf.info())\n",
    "print(fulldf.shape)\n",
    "print(fulldf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "sorted_NAs = msno.nullity_sort(fulldf, sort='ascending')\n",
    "print(sorted_NAs.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missings on all new col's: 101149703 (195914.88 funding, 1 organization), 101101973 (37875973.46 funding, 89 organizations) 101041246\n",
    "# (1395888.00 funding, 1 organization)\n",
    "# Drop these two for now\n",
    "fulldf = fulldf[~fulldf[\"id\"].isin([101149703, 101101973])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When country or nutscode our NA python thinks its a NAN so have to change this behaviour\n",
    "# Also missing on all: 101041246 (1395888.00, 1 organization)\n",
    "# Missing only on scientific domain: 101120060, 101129910, 101113553, 101100494, 101081937\n",
    "# for organization order 14 of id 101120060: no nutscode and lots of missings for adress (col's dropped earlier)\n",
    "\n",
    "sorted_NAs = msno.nullity_sort(fulldf, sort='ascending')\n",
    "print(sorted_NAs.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a204a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = fulldf[~fulldf[\"id\"].isin([101041246, 101120060, 101129910, 101113553, 101100494])]\n",
    "# Ensure that 'NA' in 'country' or 'nutsCode' columns is treated as a string value and not as NaN (can't seem to fix that)\n",
    "\n",
    "sorted_NAs = msno.nullity_sort(fulldf, sort='ascending')\n",
    "print(sorted_NAs.head(20))\n",
    "\n",
    "# Outcome: ecMaxContribution\n",
    "# predictors: endDat-startDate, legalbasis, fundingScheme, expected_impact, scientific_domain, sustainability, main_topics, activityType, SME, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d6f79-d01a-457f-b15f-7480304b9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_NAs.to_csv('sorted_NAs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1438a92-497a-4638-b1fb-3a8e8fd635e4",
   "metadata": {},
   "source": [
    "## Trying some analyses ##\n",
    "First analysis: linear regression supervised learning method. main_topics and expected_impact are processed using MultiLabelBinarizer. The main_topics had a lot of different topics, so I put those that only occured 0.1% at max in other. I use dimensionality reduction using truncatedSVD for expected_impact and main_topics after processing them using MultiLabelBinarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bddeb86-3f94-4b83-a36c-fa9f389b7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sorted_NAs = pd.read_csv('sorted_NAs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a047649-14bf-47b4-b9a1-2a7d380d64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio for main_topics SVD (40 components): 0.7114\n",
      "Explained variance ratio for expected_impact SVD (3 components): 0.8336\n",
      "Mean Squared Error: 1104861849714.09\n",
      "Root Mean Squared Error: 1051124.09\n",
      "R-squared: 1.00\n",
      "\n",
      "Model components saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- RUN THIS PART ONCE TO TRAIN AND SAVE YOUR MODEL COMPONENTS ---\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import joblib # To save/load your model and transformers\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df = sorted_NAs.copy()\n",
    "\n",
    "# Ensure scientific_domain is lowercase\n",
    "df['scientific_domain'] = df['scientific_domain'].str.lower()\n",
    "\n",
    "# Convert 'SME' to numerical (assuming it's boolean or similar)\n",
    "df['SME'] = df['SME'].astype(int)\n",
    "\n",
    "# MultiLabelBinarizer for main_topics with grouping\n",
    "mlb_topics = MultiLabelBinarizer(sparse_output=True)\n",
    "df['main_topics'] = df['main_topics'].astype('object')\n",
    "df['main_topics'] = df['main_topics'].apply(\n",
    "    lambda x: [str(i).lower() for i in (literal_eval(x) if isinstance(x, str) and x.startswith('[') else x)]\n",
    "    if isinstance(x, (str, list)) else []\n",
    ")\n",
    "\n",
    "# Calculate frequency of each topic\n",
    "all_topics = [topic for sublist in df['main_topics'] for topic in sublist]\n",
    "topic_counts = pd.Series(all_topics).value_counts()\n",
    "total_count = len(all_topics)\n",
    "\n",
    "# Define a frequency threshold (e.g., 0.1%)\n",
    "threshold = 0.001\n",
    "infrequent_topics = topic_counts[topic_counts / total_count < threshold].index.tolist()\n",
    "\n",
    "# Replace infrequent topics with \"other\"\n",
    "df['main_topics'] = df['main_topics'].apply(\n",
    "    lambda topics: [topic if topic not in infrequent_topics else \"other\" for topic in topics]\n",
    ")\n",
    "\n",
    "# Apply MultiLabelBinarizer\n",
    "topic_labels = mlb_topics.fit_transform(df['main_topics'])\n",
    "topic_columns = [f'topic_{label.replace(' ', '_').replace('-', '_')}' for label in mlb_topics.classes_]\n",
    "topic_df = pd.DataFrame.sparse.from_spmatrix(topic_labels, columns=topic_columns)\n",
    "\n",
    "# Apply SVD with 40 components for main_topics\n",
    "svd_topics = TruncatedSVD(n_components=40, random_state=42)\n",
    "topic_svd = svd_topics.fit_transform(topic_df)\n",
    "explained_variance_topics = svd_topics.explained_variance_ratio_.sum()\n",
    "print(f'Explained variance ratio for main_topics SVD (40 components): {explained_variance_topics:.4f}')\n",
    "\n",
    "topic_svd_df = pd.DataFrame(topic_svd, columns=[f'topic_svd_{i+1}' for i in range(topic_svd.shape[1])])\n",
    "df = pd.concat([df.reset_index(drop=True), topic_svd_df], axis=1)\n",
    "\n",
    "# Drop the original 'main_topics' column\n",
    "df = df.drop(columns=['main_topics'])\n",
    "\n",
    "# MultiLabelBinarizer for expected_impact\n",
    "mlb_impact = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "# Function to safely parse and lowercase the impact strings\n",
    "def parse_impact(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip() # Remove any leading/trailing whitespace\n",
    "        if x.startswith('[') and x.endswith(']'):\n",
    "            try:\n",
    "                # Safely evaluate string representation of a list\n",
    "                return [str(i).lower() for i in literal_eval(x)]\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Fallback if literal_eval fails (e.g., malformed list string)\n",
    "                return [x.lower()] if x else []\n",
    "        elif x == 'Confidential':\n",
    "            return [] # Treat 'Confidential' as no specific impact\n",
    "        else:\n",
    "            # For single words like 'societal' or 'technological'\n",
    "            return [x.lower()] if x else []\n",
    "    elif isinstance(x, list):\n",
    "        # If it's already a list, just lowercase elements\n",
    "        return [str(i).lower() for i in x]\n",
    "    else:\n",
    "        return [] # Handle any other unexpected types\n",
    "\n",
    "df['expected_impact_processed'] = df['expected_impact'].apply(parse_impact)\n",
    "\n",
    "# Fit and transform with MultiLabelBinarizer\n",
    "impact_labels = mlb_impact.fit_transform(df['expected_impact_processed'])\n",
    "\n",
    "# Create column names, ensuring they are valid and descriptive\n",
    "impact_columns = [f'impact_{label.replace(\" \", \"_\").replace(\"-\", \"_\").lower()}' for label in mlb_impact.classes_]\n",
    "impact_df = pd.DataFrame.sparse.from_spmatrix(impact_labels, columns=impact_columns)\n",
    "\n",
    "# --- Apply SVD with 3 components for expected_impact ---\n",
    "svd_impact = TruncatedSVD(n_components=3, random_state=42)\n",
    "impact_svd = svd_impact.fit_transform(impact_df)\n",
    "explained_variance_impact = svd_impact.explained_variance_ratio_.sum()\n",
    "print(f'Explained variance ratio for expected_impact SVD (3 components): {explained_variance_impact:.4f}')\n",
    "\n",
    "impact_svd_df = pd.DataFrame(impact_svd, columns=[f'impact_svd_{i+1}' for i in range(impact_svd.shape[1])])\n",
    "df = pd.concat([df.reset_index(drop=True), impact_svd_df], axis=1)\n",
    "\n",
    "# Drop the original 'expected_impact' column and the intermediate processed column\n",
    "df = df.drop(columns=['expected_impact', 'expected_impact_processed'])\n",
    "\n",
    "# Calculate duration in days\n",
    "df['startDate'] = pd.to_datetime(df['startDate'])\n",
    "df['endDate'] = pd.to_datetime(df['endDate'])\n",
    "df['duration'] = (df['endDate'] - df['startDate']).dt.days\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "topic_columns_svd = [col for col in df.columns if col.startswith('topic_svd_')]\n",
    "impact_columns_svd = [col for col in df.columns if col.startswith('impact_svd_')]\n",
    "\n",
    "X = df[['duration', 'legalBasis', 'fundingScheme', 'scientific_domain', 'sustainability', 'activityType', 'SME', 'country'] + topic_columns_svd + impact_columns_svd]\n",
    "y = df['ecMaxContribution']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['legalBasis', 'fundingScheme', 'scientific_domain', 'activityType', 'country']\n",
    "numerical_features = ['duration', 'sustainability', 'SME'] + topic_columns_svd + impact_columns_svd\n",
    "\n",
    "# Data Preprocessing using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Model Selection and Training (Random Forest as an example)\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create a pipeline to combine preprocessing and model training\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "\n",
    "# --- Save the trained components ---\n",
    "joblib.dump(pipeline, 'prediction_pipeline.joblib')\n",
    "joblib.dump(mlb_topics, 'mlb_topics.joblib')\n",
    "joblib.dump(svd_topics, 'svd_topics.joblib')\n",
    "joblib.dump(mlb_impact, 'mlb_impact.joblib')\n",
    "joblib.dump(svd_impact, 'svd_impact.joblib')\n",
    "joblib.dump(infrequent_topics, 'infrequent_topics.joblib')\n",
    "joblib.dump(X.columns.tolist(), 'feature_columns.joblib') # Save feature order\n",
    "print(\"\\nModel components saved successfully!\")\n",
    "\n",
    "# --- END OF TRAINING & SAVING SCRIPT ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92579906-937d-4d6b-8651-bc3552c22413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
